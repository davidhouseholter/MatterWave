# Dynamic Negative Hardening (DNH)

This document specifies the Dynamic Negative Hardening (DNH) component for KG-CAE: a curriculum and toolkit for moving negative sampling from easy (random) to hard (graph-swapped, latent-nearest different-URI) during training. DNH is intended as a modular sampler and curriculum policy that can be plugged into existing contrastive/triplet training loops.

## 1. Motivation

Hard negatives are far more informative than random negatives for representation learning. However, training exclusively with the hardest negatives can slow convergence or cause instability early in training. DNH uses a paced curriculum that increases negative difficulty as the model matures.

Goals:
- Improve factual sensitivity (reduce hallucination) by exposing the model to negatives that change knowledge-graph facts while keeping surface text similar.
- Speed up convergence by gradually increasing negative difficulty.
- Provide diagnostics and ablations to study negative-source complementarity.

## 2. Negative Sources

1. Random Negatives (easy): uniformly sample other examples in the batch or dataset. Useful early in training for stable gradients.
2. Graph-Swapped Negatives (medium): create negatives by replacing one or more entity URIs in the RDF context with a same-type neighbor (e.g., swap A with B where A and B share rdf:type). This keeps surface tokens similar but changes factual correctness.
3. Latent-Nearest Different-URI Negatives (hard): mine nearest neighbors in embedding space that have a different ground-truth URI. Requires a nearest-neighbor index (FAISS/Annoy/HNSW) and periodic indexing.
4. Adversarial/Contrastive-Synth Negatives (optional): small paraphrase edits generated by an LLM or gradient-based adversary that flip the classification but keep lexical similarity.

## 3. Curriculum Schedules

We recommend several curriculum schedules; each has hyperparameters to tune (warmup epochs, transition slope, final mixing ratios):

- Linear schedule: start with p_random=1.0, linearly reduce to p_random=0 over warmup epochs E_warm, while increasing p_graph and p_latent.
- Step schedule: keep random only for E_ign epochs, switch to graph-swap for E_step, then enable latent-hard after E_step+E_hard.
- Performance-triggered schedule: monitor validation clustering metric or margin violation rate; when metric crosses threshold, increase hardness.

A small example (recommended default): E_warm=2, E_step=5, after epoch 5 enable latent mining with p_mix = {random:0.2, graph:0.5, latent:0.3}.

## 4. Sampler API (conceptual)

class DNHSampler:
    """Construct negatives given a batch of anchor examples.

    Inputs:
    - batch: list of examples with fields (text, rdf_context, uri_id)
    - epoch: current epoch
    - model: optional model to compute embeddings (for latent negatives)

    Methods:
    - sample_negatives(batch) -> list of negative examples per anchor
    - update_index(embeddings) -> rebuilds FAISS/HNSW index periodically
    """

Implementation notes:
- Maintain an embedding index that is refreshed every R steps (R configurable). To avoid stale negatives, use a ring buffer of recent embeddings or asynchronous indexing.
- For graph-swapped negatives, constrain swaps by type or by ontological plausibility (e.g., only swap to same rdf:type or within same domain).
- Ensure sampled latent-nearest negatives have a different ground-truth URI; if none found within top-K, fall back to graph-swap or random negative.

## 5. Loss Integration Patterns

DNH works with common losses:
- Triplet loss: anchor-positive distance vs anchor-negative distance; negatives come from DNHSampler.
- InfoNCE / contrastive loss: form positive and multiple negatives per anchor using DNH mixes.
- Hybrid: use cross-entropy classification head plus contrastive term with DNH negatives for the contrastive portion.

When using classification head, consider upweighting losses from hard-negative batches to emphasize factual discrimination.

## 6. Diagnostics and Metrics

Track these during training:
- Margin violation rate (fraction of triplets where d(a,p) + margin > d(a,n)).
- Hard-negative hit rate: fraction of mined latent neighbors that differ in URI.
- Stability metrics: gradient norm, training loss spikes after schedule transitions.
- Downstream metrics: entity linking accuracy, embedding-factuality AUC on holdout adversarial negatives.

Visualizations:
- Curriculum timeline: stacked area chart showing fraction of negative sources over epochs.
- Nearest-neighbor confusion matrix: show top-5 mined negatives and their URIs.

## 7. Ablations and Experiments

Experiment matrix:
- Baseline: static random negatives.
- Graph-only: random + graph-swap static mix.
- Latent-only: random + latent-nearest static mix.
- Full DNH: curriculum schedules (linear/step/perf-triggered).

Evaluate on:
- Convergence speed (validation loss vs steps).
- Final entity linking accuracy and MRR.
- Robustness to paraphrase and adversarial edits.

## 8. Code Sketch (PyTorch-flavored)

def sample_for_batch(batch, epoch, model, index, schedule):
    # Pseudocode: returns negatives per anchor
    # ...existing code...
    return negatives

## 9. Operational Considerations

- Index freshness vs cost: frequent reindexing improves negative quality but increases compute; tune R to balance.
- Memory: FAISS index and embedding cache can be large â€” consider product quantization (PQ) or HNSW with on-disk persistence.
- Reproducibility: save sampler seeds and schedule checkpoints to reproduce exact curriculum.

## 10. Next steps / Implementation Checklist

- Implement `DNHSampler` with configurable schedules.
- Add tests: synthetic dataset with known nearest neighbors to validate latent mining and graph-swap correctness.
- Integrate with training loop and add metrics logging (MLflow/Weights & Biases).
- Run ablation suite and capture curriculum plots.

---
